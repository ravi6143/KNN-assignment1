{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b51287f8-0440-4189-b15d-c429ffa6332c",
   "metadata": {},
   "source": [
    "## Question - 1\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e922c-acaa-44e4-a61a-6eecbb92031a",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a simple yet powerful supervised machine learning algorithm used for both classification and regression tasks. In KNN, the prediction for a new data point is based on the majority class (for classification) or the average value (for regression) of its K nearest neighbors in the feature space.\n",
    "\n",
    "\n",
    "## 1 Store Training Data:\n",
    "\n",
    "KNN memorizes the entire training dataset without performing any explicit training phase.\n",
    "\n",
    "\n",
    "## 2 Calculate Distances:\n",
    "\n",
    "To make predictions for a new data point, KNN calculates the distances between that point and every other point in the training dataset.\n",
    "The most common distance metric used is Euclidean distance, but other metrics like Manhattan distance can also be used.\n",
    "\n",
    "\n",
    "## 3 Find Nearest Neighbors:\n",
    "\n",
    "After calculating distances, KNN identifies the K nearest neighbors of the new data point based on the distance metric.\n",
    "The value of K is a hyperparameter that needs to be specified before applying the algorithm.\n",
    "\n",
    "\n",
    "## 4 Make Prediction:\n",
    "\n",
    "For classification, KNN assigns the majority class label among the K nearest neighbors to the new data point.\n",
    "For regression, KNN computes the average (or weighted average) of the labels of the K nearest neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8b7add-613d-4e59-b52d-e0d5832c4f12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12aa9490-f112-4ad5-8785-357901a65c81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2d1fbf-32c5-4401-a566-87f112919cd3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9ef7044f-e101-48e9-8430-322fc7e1eb7a",
   "metadata": {},
   "source": [
    "## Questin - 2\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b7134b-a1ae-44c3-8e9f-d521a9c869ca",
   "metadata": {},
   "source": [
    "## 1 Cross-Validation:\n",
    "\n",
    "Use cross-validation techniques, such as k-fold cross-validation, to evaluate the performance of the KNN algorithm for different values of K.\n",
    "Split the dataset into training and validation sets, and train the KNN model using various values of K.\n",
    "Evaluate the model's performance on the validation set using appropriate evaluation metrics (e.g., accuracy, F1-score for classification, mean squared error for regression).\n",
    "Choose the value of K that results in the best performance on the validation set.\n",
    "\n",
    "\n",
    "\n",
    "## 2 Rule of Thumb:\n",
    "\n",
    "A common rule of thumb is to choose the value of K as the square root of the number of data points in the training set.\n",
    "For smaller datasets, lower values of K (e.g., 1, 3, 5) are often preferred to prevent overfitting and capture local patterns in the data.\n",
    "For larger datasets, higher values of K may be more appropriate to smooth out noisy data and reduce variance.\n",
    "\n",
    "\n",
    "## 3 Odd Values:\n",
    "\n",
    "It's generally recommended to choose an odd value of K to avoid ties in the voting process, especially for binary classification tasks.\n",
    "Odd values of K ensure that there is always a clear majority when determining the class label of a data point.\n",
    "\n",
    "\n",
    "## 4 Domain Knowledge:\n",
    "\n",
    "Consider the characteristics of the dataset and the problem domain when choosing the value of K.\n",
    "For example, if the dataset exhibits strong local patterns or clusters, smaller values of K may be more suitable.\n",
    "Conversely, if the dataset is noisy or contains outliers, larger values of K may help to smooth out the predictions.\n",
    "\n",
    "\n",
    "## 5 Grid Search:\n",
    "\n",
    "Perform a grid search over a predefined range of K values to find the optimal value through an exhaustive search.\n",
    "Define a range of candidate values for K (e.g., 1 to 20) and evaluate the model's performance for each value using cross-validation.\n",
    "Choose the value of K that yields the best performance based on the chosen evaluation metric.\n",
    "\n",
    "\n",
    "## 6 Visual Inspection:\n",
    "\n",
    "Visualize the decision boundaries of the KNN model for different values of K to gain insights into how the choice of K affects the model's behavior.\n",
    "Plot the decision boundaries along with the data points and observe how they change as K varies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09d5962-3a81-4504-b158-ab203c19e1c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e03481-8b40-4510-bc89-3d3dec6419a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f1366f-0b1c-4eb9-8e90-b0ed3e7ff0a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "58c72865-dc1a-4ca8-b9dc-54381702c64d",
   "metadata": {},
   "source": [
    "## Question - 3\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57889b98-3c2d-4232-9be1-7687971f27df",
   "metadata": {},
   "source": [
    "## KNN Classifier:\n",
    "\n",
    "* KNN classifier is used for classification tasks, where the target variable is categorical or discrete.\n",
    "\n",
    "* The prediction made by a KNN classifier is the majority class label among the K nearest neighbors of the query point.\n",
    "\n",
    "* In KNN classification, the class labels of the nearest neighbors are used to determine the class membership of the query point.\n",
    "\n",
    "* Typical evaluation metrics for KNN classification include accuracy, precision, recall, F1-score, and confusion matrix.\n",
    "\n",
    "\n",
    "## KNN Regressor:\n",
    "\n",
    "* KNN regressor is used for regression tasks, where the target variable is continuous or numeric.\n",
    "\n",
    "* The prediction made by a KNN regressor is the average (or weighted average) of the target variable among the K nearest neighbors of the query point.\n",
    "\n",
    "* In KNN regression, the target variable values of the nearest neighbors are used to estimate the continuous value of the query point.\n",
    "\n",
    "* Typical evaluation metrics for KNN regression include mean squared error (MSE), root mean squared error (RMSE), mean absolute error (MAE), and R-squared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3966712e-cd10-4b7f-ac40-6ed26b082246",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c9687b-d7a3-45c6-a9a6-65b5dd9912e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dee26525-8352-4853-b199-ef20583798c8",
   "metadata": {},
   "source": [
    "The performance of a K-Nearest Neighbors (KNN) algorithm can be measured using various evaluation metrics, depending on whether it's applied for classification or regression tasks. Here's how you can measure the performance of KNN:\n",
    "\n",
    "## For Classification Tasks:\n",
    "\n",
    "\n",
    "1. Accuracy:\n",
    "\n",
    "* Accuracy measures the proportion of correctly classified instances out of all instances.\n",
    "\n",
    "* It's calculated as the ratio of the number of correct predictions to the total number of predictions.\n",
    "\n",
    "* Accuracy= TP+TN/TP+TN+FP+FN\n",
    "\n",
    "where TP is true positives, TN is true negatives, FP is false positives, and FN is false negatives.\n",
    "\n",
    "\n",
    "\n",
    "2. Precision, Recall, and F1-score:\n",
    "\n",
    "* Precision measures the proportion of correctly predicted positive instances out of all instances predicted as positive.\n",
    "\n",
    "* Recall (or sensitivity) measures the proportion of correctly predicted positive instances out of all actual positive instances.\n",
    "\n",
    "* F1-score is the harmonic mean of precision and recall, providing a balance between the two metrics.\n",
    "\n",
    "* These metrics are particularly useful in imbalanced datasets.\n",
    "\n",
    "\n",
    "3. Confusion Matrix:\n",
    "\n",
    "* A confusion matrix provides a more detailed breakdown of the model's performance by showing the counts of true positives, true negatives, false positives, and false negatives.\n",
    "\n",
    "* It's useful for understanding where the model is making errors and which classes are being confused.\n",
    "\n",
    "\n",
    "\n",
    "## For Regression Tasks:\n",
    "\n",
    "1. Mean Squared Error (MSE):\n",
    "\n",
    "* MSE measures the average squared difference between the predicted values and the actual values.\n",
    "\n",
    "* It gives higher weight to large errors and is sensitive to outliers.\n",
    "\n",
    "2. Root Mean Squared Error (RMSE):\n",
    "\n",
    "* RMSE is the square root of the MSE, providing a measure of the average error in the same units as the target variable.\n",
    "\n",
    "* It's easier to interpret than MSE as it's in the same scale as the target variable.\n",
    "\n",
    "3. Mean Absolute Error (MAE):\n",
    "\n",
    "* MAE measures the average absolute difference between the predicted values and the actual values.\n",
    "\n",
    "* It gives equal weight to all errors and is less sensitive to outliers.\n",
    "\n",
    "4. R-squared (R2):\n",
    "\n",
    "* R-squared measures the proportion of the variance in the target variable that is explained by the model.\n",
    "\n",
    "* It ranges from 0 to 1, where 1 indicates a perfect fit and 0 indicates no improvement over a baseline model.\n",
    "\n",
    "* It's useful for understanding how well the model fits the data compared to a simple baseline model.## Question - 4\n",
    "ans - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb75fbbb-86e5-4f0f-8679-a715175b9df7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c76bc61-d1be-48cf-9a91-9f946fcfae29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e24b1f8e-9e1c-4d3e-90ea-b6f219eb5680",
   "metadata": {},
   "source": [
    "## Question - 5\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06715a2-280c-4581-a6af-85c7fc960b65",
   "metadata": {},
   "source": [
    "The curse of dimensionality refers to the phenomenon where the performance of certain algorithms, including K-Nearest Neighbors (KNN), deteriorates as the dimensionality of the feature space increases. In the context of KNN, the curse of dimensionality manifests in several ways:\n",
    "\n",
    "1.Increased Sparsity:\n",
    "\n",
    "As the number of dimensions increases, the volume of the feature space grows exponentially.\n",
    "With a fixed number of data points, the density of data points in the feature space decreases, leading to increased sparsity.\n",
    "This sparsity makes it more difficult for KNN to find a sufficient number of nearest neighbors to make accurate predictions.\n",
    "Increased Distance:\n",
    "\n",
    "In high-dimensional spaces, the notion of distance becomes less meaningful.\n",
    "Euclidean distance, which is commonly used in KNN, becomes less discriminative as the number of dimensions increases.\n",
    "Points that are close in Euclidean space may not be close in high-dimensional space, leading to inaccuracies in nearest neighbor search.\n",
    "Overfitting:\n",
    "\n",
    "With a large number of dimensions, the model may become overly sensitive to noise and outliers in the data.\n",
    "KNN relies on local information from neighboring data points, and in high-dimensional spaces, the concept of \"local\" becomes less meaningful.\n",
    "The model may capture noise in the training data instead of meaningful patterns, leading to overfitting and poor generalization to unseen data.\n",
    "Computational Complexity:\n",
    "\n",
    "Nearest neighbor search becomes computationally expensive in high-dimensional spaces.\n",
    "The time required to find the nearest neighbors increases exponentially with the number of dimensions, making KNN impractical for large-dimensional datasets.\n",
    "Curse of Sampling:\n",
    "\n",
    "As the dimensionality increases, the amount of data required to maintain the same level of statistical significance also increases exponentially.\n",
    "Collecting sufficient data to cover the high-dimensional space becomes increasingly challenging and resource-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096afb4-fab9-4588-a3aa-a0ac1e7e08ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8063e64c-8c1d-4abc-b812-d9674f968712",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c8639010-2317-4dbf-a4af-c9d36e16a901",
   "metadata": {},
   "source": [
    "## Question - 6\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79dfcfe2-32d3-47b0-8d77-4a0bd93f2c96",
   "metadata": {},
   "source": [
    "Here are some ways to handle missing values in KNN:\n",
    "\n",
    "1. Imputation: One common approach is to impute missing values with estimates, such as the mean or median value of the feature, or a regression-based estimate using other non-missing features. ...\n",
    "\n",
    "2. Deletion: Another approach is to delete samples with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24a91d2-384d-442a-93fd-ef68a92ad851",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdec540-330b-4f46-91b3-7e89f094699f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "67137db8-72c4-4389-8357-7b3ad59943a4",
   "metadata": {},
   "source": [
    "## Question - 7\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93066ae-f0dd-412f-98e2-3224dbe2dcb5",
   "metadata": {},
   "source": [
    "## KNN Classifier:\n",
    "\n",
    "* Purpose: It's used when you want to put things into categories, like whether an email is spam or not.\n",
    "\n",
    "* Output: It gives you a category label, like \"spam\" or \"not spam\".\n",
    "\n",
    "* How it Works: It looks at the closest neighbors to the new point and sees which category they belong to. Then, it assigns the new point to the most common category among its neighbors.\n",
    "\n",
    "\n",
    "## KNN Regressor:\n",
    "\n",
    "* Purpose: It's used when you want to predict a number, like the price of a house.\n",
    "\n",
    "* Output: It gives you a number, like the predicted price of the house.\n",
    "\n",
    "* How it Works: It looks at the closest neighbors to the new point and takes the average (or weighted average) of their values to predict the new point's value.\n",
    "\n",
    "\n",
    "## Comparison:\n",
    "\n",
    "* Performance:\n",
    "\n",
    "Classifier is good when you have clear boundaries between categories.\n",
    "\n",
    "Regressor is good when you want to predict something continuous.\n",
    "\n",
    "* Sensitivity to Noise:\n",
    "\n",
    "Classifier can be thrown off by noisy data.\n",
    "\n",
    "Regressor can handle noise a bit better because it averages nearby points.\n",
    "\n",
    "* Interpretability:\n",
    "Both are relatively easy to understand: Classifier says which category something belongs to, and Regressor gives a predicted number.\n",
    "\n",
    "\n",
    "## Choosing Between Them:\n",
    "\n",
    "1. Use classifier for sorting things into categories, like classifying emails.\n",
    "2. Use regressor for predicting numbers, like predicting prices or temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eefefec-b6dd-49fd-ae55-fb2199758c82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df2b0f5b-765e-41d6-89a6-0b88a1569825",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4e884a-73f0-4ddc-8b83-2f8899cab3b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c01e5caa-7b11-42d2-b55b-4127d9a91230",
   "metadata": {},
   "source": [
    "## Question - 8\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53504f8c-6daf-4551-989d-5370bb9bcf03",
   "metadata": {},
   "source": [
    "## Strengths of KNN:\n",
    "\n",
    "1. Simple to Understand and Implement: KNN is easy to understand and implement, making it suitable for beginners and quick prototyping.\n",
    "\n",
    "2. Non-parametric: KNN is a non-parametric algorithm, meaning it makes no assumptions about the underlying data distribution. This flexibility allows it to adapt to different types of data.\n",
    "\n",
    "3. No Training Phase: Unlike many other algorithms, KNN does not require a training phase. The model learns directly from the training data and makes predictions based on similarities to neighboring data points.\n",
    "\n",
    "\n",
    "4. Effective with Locally Smooth Data: KNN performs well with locally smooth data, where neighboring points tend to have similar target values or class labels.\n",
    "\n",
    "\n",
    "\n",
    "## Weaknesses of KNN:\n",
    "\n",
    "1. Computational Complexity: KNN can be computationally expensive, especially with large datasets or high-dimensional feature spaces, as it requires calculating distances between the new point and all training data points.\n",
    "\n",
    "2. Sensitive to Outliers and Irrelevant Features: Outliers or irrelevant features can significantly affect KNN's performance, as it relies heavily on distance metrics.\n",
    "\n",
    "3. Need for Feature Scaling: KNN is sensitive to the scale of features, so feature scaling (e.g., normalization or standardization) is often necessary to ensure all features contribute equally to distance calculations.\n",
    "\n",
    "4. Memory Requirements: KNN requires storing the entire training dataset in memory, which can be memory-intensive for large datasets.\n",
    "\n",
    "## Addressing Weaknesses of KNN:\n",
    "\n",
    "* Dimensionality Reduction: To address computational complexity with high-dimensional data, dimensionality reduction techniques like Principal Component Analysis (PCA) or t-distributed Stochastic Neighbor Embedding (t-SNE) can be used to reduce the number of features while preserving relevant information.\n",
    "\n",
    "* Feature Selection: Removing irrelevant or redundant features can improve KNN's performance and reduce sensitivity to outliers.\n",
    "\n",
    "* Distance Metric Selection: Choosing an appropriate distance metric based on the characteristics of the data can help mitigate the impact of outliers and irrelevant features. For example, using robust distance metrics like Manhattan distance or Mahalanobis distance may be more suitable in certain cases.\n",
    "\n",
    "* Lazy Learning Techniques: Implementations of KNN often include optimizations such as KD-trees or ball trees to speed up nearest neighbor search and reduce computational overhead.\n",
    "\n",
    "* Ensemble Methods: Combining multiple KNN models through techniques like KNN bagging or KNN boosting can improve predictive performance and reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899a2a03-013f-484c-9fe6-1702832efa56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4ca6f6-6647-4c46-bf8a-53dc8146d323",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297f65e-80fd-4699-ae61-436ad91c7c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5d1bd1d3-a0a0-4fd9-aa96-dae2deb053bd",
   "metadata": {},
   "source": [
    "## Question - 9\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c81e60-e92a-4c9a-95fc-425248ee1991",
   "metadata": {},
   "source": [
    "## Euclidean Distance:\n",
    "\n",
    "1. Euclidean distance is the straight-line distance between two points in a Euclidean space.\n",
    "\n",
    "2. It is calculated as the square root of the sum of squared differences between corresponding coordinates of two points.\n",
    "\n",
    "3. In two-dimensional space (e.g., with x and y coordinates), Euclidean distance is computed as:\n",
    "\n",
    "## Euclidean distance = sqrt((x2 - x1)^2 + (y2 - y1)^2)\n",
    " \n",
    "4. Euclidean distance considers the actual geometric distance between points and is sensitive to differences in all dimensions.\n",
    "\n",
    "\n",
    "## Manhattan Distance:\n",
    "\n",
    "1. Manhattan distance, also known as city block distance or taxicab distance, measures the distance between two points along axes at right angles.\n",
    "\n",
    "2. It is calculated as the sum of the absolute differences between corresponding coordinates of two points.\n",
    "\n",
    "3. In two-dimensional space, Manhattan distance is computed as:\n",
    "\n",
    "## Manhattan distance = |x2 - x1| + |y2 - y1| \n",
    "\n",
    "4. Manhattan distance represents the distance a car would have to travel along the grid-like streets of a city to reach from one point to another, moving only horizontally and vertically.\n",
    "\n",
    "\n",
    "\n",
    "## Differences:\n",
    "\n",
    "* Directionality: Euclidean distance considers the straight-line distance between points, while Manhattan distance measures the distance along axes, making right-angle turns.\n",
    "\n",
    "* Sensitivity: Euclidean distance is more sensitive to differences in all dimensions, while Manhattan distance may be more robust to outliers or differences in one dimension.\n",
    "\n",
    "* Dimensionality: Both metrics can be used in any dimension, but the interpretation of distance differs. Euclidean distance represents true geometric distance, while Manhattan distance represents the distance traveled along axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df38a18e-0132-478c-8635-5f4bb273e7d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96dec1f8-1cf6-47c0-83ca-894b31280057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a113e88-f562-46cb-bdfc-7a1cb8e5b8ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0d7b09-c3f3-4404-9eeb-0e8a9cc2f57a",
   "metadata": {},
   "source": [
    "## Question - 10\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce46b63a-fd1a-4f0d-abc2-4132c1ff04f0",
   "metadata": {},
   "source": [
    "Feature scaling plays a crucial role in the K-Nearest Neighbors (KNN) algorithm. Here's why it's important and what role it serves:\n",
    "\n",
    "## 1 Equalizing Feature Magnitudes:\n",
    "\n",
    "Feature scaling ensures that all features contribute equally to the distance calculations in KNN. Without scaling, features with larger magnitudes can dominate the distance calculations, leading to biased results.\n",
    "\n",
    "\n",
    "## 2 Improving Performance:\n",
    "\n",
    "Scaling features can improve the performance and accuracy of the KNN algorithm by reducing the impact of features with larger scales and preventing distance metrics from being skewed.\n",
    "\n",
    "\n",
    "## 3 Avoiding Bias:\n",
    "\n",
    "KNN calculates distances between data points based on the features' values. If one feature has a larger scale (e.g., age in years vs. income in thousands of dollars), the distance metric may be biased towards that feature, leading to inaccurate predictions.\n",
    "\n",
    "\n",
    "## 4 Maintaining Consistency:\n",
    "\n",
    "Feature scaling ensures that changes in one feature have a consistent impact on distance calculations, regardless of the feature's scale. This maintains the integrity of the distance metric and ensures fair comparisons between data points.\n",
    "\n",
    "\n",
    "## 5 Handling Different Units:\n",
    "\n",
    "Feature scaling is particularly important when dealing with features measured in different units or scales. Scaling brings all features to a similar scale, making them directly comparable and facilitating meaningful distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53919a6-7883-45ef-80c1-fd02a107f9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
